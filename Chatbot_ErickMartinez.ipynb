{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ErickMartinezz/ProcesamientodelHabla/blob/main/Chatbot_ErickMartinez.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hghx5BwKdm-A",
        "outputId": "71f50d8e-7d7b-49f5-fbd1-339d76ee6b4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n",
            "\u001b[38;5;3mâš  Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy --quiet\n",
        "!python -m spacy download es_core_news_sm --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9SRh-eDdpWp",
        "outputId": "44edddf1-4dc6-43f2-f94b-d7782bb5d193"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('Esto', 'PRON'), ('es', 'AUX'), ('una', 'DET'), ('frase', 'NOUN'), ('.', 'PUNCT')]\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"es_core_news_sm\")\n",
        "import es_core_news_sm\n",
        "nlp = es_core_news_sm.load()\n",
        "doc = nlp(\"Esto es una frase.\")\n",
        "print([(w.text, w.pos_) for w in doc])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qr3pyit4kWQn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnrG-1ylkncZ"
      },
      "source": [
        "# Chatbots basados en recuperaciÃ³n\n",
        "\n",
        "En inglÃ©s information retrieval chatbots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3bs1FmWkfrz"
      },
      "source": [
        "# Motor de bÃºsqueda"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enbz9kXGkWlx"
      },
      "source": [
        "* BÃºsqueda por palabras clave: Extrae palabras clave de la pregunta del usuario y busca coincidencias en las preguntas almacenadas.\n",
        "\n",
        "* Similitud del coseno: Si has representado las preguntas como vectores (por ejemplo, usando TF-IDF o word embeddings), puedes usar la similitud del coseno para medir la distancia entre las preguntas.\n",
        "\n",
        "* Word embeddings: Utiliza modelos de word embeddings como Word2Vec o BERT para obtener representaciones semÃ¡nticas de las preguntas y las consultas del usuario."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQoVRp4gjxUo"
      },
      "source": [
        "### BÃºsqueda por palabras claves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3BljEMEOhpTO"
      },
      "outputs": [],
      "source": [
        "tu_diccionario = {\n",
        "   \"hola\": \"Â¡Hola! Â¿En quÃ© puedo ayudarte?\",\n",
        "   \"adiÃ³s\": \"Hasta luego. Â¡Que tengas un buen dÃ­a!\",\n",
        "   \"informaciÃ³n\": \"Â¿QuÃ© tipo de informaciÃ³n estÃ¡s buscando?\",\n",
        "   # Agrega mÃ¡s entradas de diccionario segÃºn tus necesidades\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z4q_k1_3hvyz"
      },
      "outputs": [],
      "source": [
        "def responder_pregunta(pregunta):\n",
        "    pregunta_procesada = nlp(pregunta.lower())  # Procesa la pregunta y convierte a minÃºsculas\n",
        "    respuesta = \"Lo siento, no entiendo tu pregunta.\"\n",
        "\n",
        "    # Busca una coincidencia en el diccionario\n",
        "    for palabra in pregunta_procesada:\n",
        "        # regresa la primer coincidencia que encuentra\n",
        "        if palabra.text in tu_diccionario:\n",
        "            respuesta = tu_diccionario[palabra.text]\n",
        "            break\n",
        "\n",
        "    return respuesta\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "9gMPKi-ghwcA",
        "outputId": "b409c0e2-8b83-46f8-8bc8-f7631575baf1"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-4233496462>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mentrada_usuario\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"TÃº: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mentrada_usuario\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"salir\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Chatbot: Hasta luego.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "while True:\n",
        "    entrada_usuario = input(\"TÃº: \")\n",
        "    if entrada_usuario.lower() == \"salir\":\n",
        "        print(\"Chatbot: Hasta luego.\")\n",
        "        break\n",
        "    respuesta = responder_pregunta(entrada_usuario)\n",
        "    print(\"Chatbot:\", respuesta)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6S88twrY5sOS"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nI2FsyUOj3je"
      },
      "source": [
        "## BÃºsqueda por similitud"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AoZIaX0Kj7xs"
      },
      "source": [
        "Para los chatbots basados â€‹â€‹en recuperaciÃ³n, es comÃºn utilizar bolsas de palabras (bag of words) o tf-idf para calcular la similitud de intenciones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CReIz0ISj75s"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Datos de ejemplo\n",
        "preguntas = [\"Â¿QuÃ© es el aprendizaje automÃ¡tico?\",\n",
        "             \"Â¿CÃ³mo funciona la regresiÃ³n lineal?\"]\n",
        "respuestas = [\"El aprendizaje automÃ¡tico es una rama de la inteligencia artificial...\",\n",
        "              \"La regresiÃ³n lineal es un mÃ©todo de modelado...\"]\n",
        "\n",
        "# VectorizaciÃ³n con TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(preguntas)\n",
        "\n",
        "# FunciÃ³n para encontrar la mejor coincidencia\n",
        "def responder_pregunta(consulta_usuario):\n",
        "    consulta_vec = vectorizer.transform([consulta_usuario])\n",
        "    similitudes = cosine_similarity(consulta_vec, tfidf_matrix).flatten()\n",
        "    print(similitudes)\n",
        "    indice_mejor_coincidencia = similitudes.argmax()\n",
        "    print(indice_mejor_coincidencia)\n",
        "    return respuestas[indice_mejor_coincidencia]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "foqaZ1FN583i",
        "outputId": "eaeec26f-7115-4e1a-b7a7-4e5d3e5c1620"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.4 0.6]\n",
            "1\n",
            "La regresiÃ³n lineal es un mÃ©todo de modelado...\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Ejemplo de consulta\n",
        "consulta = \"Â¿QuÃ© es la regresiÃ³n lineal?\"\n",
        "print(responder_pregunta(consulta))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20KCxfCymOHP"
      },
      "source": [
        "## BÃºsqueda por similitud en embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7g6jgLSLnilX"
      },
      "source": [
        "Puedes vectorizar el texto usando embeddings, como vimos la clase pasada.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sGxF1VglJVd"
      },
      "source": [
        "## Actividades\n",
        "\n",
        "### 1) Elaborar un dataset de preguntas y respuestas para crear un Chatbot para un aplicaciÃ³n particular. ( 3 puntos )\n",
        "\n",
        "1.1 Debe definir la aplicaciÃ³n (atenciÃ³n al cliente bancario, atenciÃ³n a estudiantes universitarios, etc).\n",
        "1.2 El listado de preguntas y respuestas debe tener como mÃ­nimo 20 elementos pregunta - respuesta.\n",
        "\n",
        "###  2) Crear el chatbot utilizando TFIDF y similitud del coseno. (1 punto)\n",
        "\n",
        "### 3) Crear otro chatbot utilizando embeddings. Indique cuÃ¡l embedding (1 punto) pre-entrenado eligiÃ³.\n",
        "\n",
        "### 4) Muestra ambos chatbots funcionando (1 punto)\n",
        "\n",
        "Adjuntar la lista de preguntas utilizadas para probar el funcionamiento.\n",
        "\n",
        "### 5) AÃ±ade tus conclusiones de todo lo realizado (2 punto)\n",
        "\n",
        "### 6) BONUS: usa lo realizado en 1 y 3 para crear un chatbot RAG. (2 puntos)\n",
        "\n",
        "* Utiliza un modelo LLM pre-entrenado.\n",
        "\n",
        "* Este punto no es obligatorio de realizar para quienes quieran regularizar / recuperar y luego rendirÃ¡n en mesa.\n",
        "* Para quienes tienen condiciones para promocionar (han realizado y entregado los TPs a tiempo) la resoluciÃ³n de este ejercicio serÃ¡ tenida en cuenta para sumar a la promociÃ³n.\n",
        "\n",
        "### 7) No olvides:\n",
        "\n",
        "* Explicar tus decisiones y configuraciones. AÃ±adir tus conclusiones.\n",
        "* Anunciar en el foro cuÃ¡l serÃ¡ tu aplicaciÃ³n y postear tu entrega y tus avances.\n",
        "* Debes subir tu notebook a un repo GitHub pÃºblico de tu propiedad compartido + enlace colab.\n",
        "* Documentar todo el proceso.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vM_8lsoNNVLJ"
      },
      "source": [
        "1) Elaborar un dataset de preguntas y respuestas para crear un Chatbot para un aplicaciÃ³n particular. ( 3 puntos )\n",
        "1.1 Debe definir la aplicaciÃ³n (atenciÃ³n al cliente bancario, atenciÃ³n a estudiantes universitarios, etc). 1.2 El listado de preguntas y respuestas debe tener como mÃ­nimo 20 elementos pregunta - respuesta."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KksRcnR6L9gz",
        "outputId": "bb356bf7-ce8f-4b35-fd31-1a458480caee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                             Pregunta         IntenciÃ³n  \\\n",
            "0            Hola, Â¿estÃ¡n abiertos?,   consulta_horario   \n",
            "1   Â¿CuÃ¡nto cuesta un lomo clÃ¡sico?,    consulta_precio   \n",
            "2              Â¿Tienen promociones?,    consulta_promos   \n",
            "3  Â¿CuÃ¡les son los mÃ©todos de pago?,      consulta_pago   \n",
            "4                    Â¿Hacen envÃ­os?,     consulta_envio   \n",
            "\n",
            "                                           Respuesta  \n",
            "0  Â¡Hola! SÃ­, estamos abiertos de 20:00 a 00:00. ...  \n",
            "1  El lomo clÃ¡sico cuesta $9500. Â¿QuerÃ©s agregar ...  \n",
            "2  SÃ­, tenemos combos especiales. Â¿QuerÃ©s que te ...  \n",
            "3  Aceptamos efectivo, tarjetas y Mercado Pago. Â¿...  \n",
            "4  SÃ­, hacemos delivery en CÃ³rdoba. Â¿En quÃ© zona ...  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# ğŸ”¹ URL del archivo en GitHub (formato raw)\n",
        "url = \"https://raw.githubusercontent.com/ErickMartinezz/ProcesamientodelHabla/main/Preguntas%20chatbot.xlsx\"\n",
        "\n",
        "# ğŸ”¸ Cargar archivo en pandas\n",
        "df = pd.read_excel(url)\n",
        "\n",
        "# Mostrar primeras filas para verificar\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLaNwj9pMTPL"
      },
      "outputs": [],
      "source": [
        "preguntas = df[\"Pregunta\"]  # Lista de preguntas\n",
        "intenciones = df[\"IntenciÃ³n\"]  # CategorÃ­as\n",
        "respuestas = df[\"Respuesta\"]  # Respuestas predefinidas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zc1MsAzTNWkv"
      },
      "source": [
        "2) Crear el chatbot utilizando TFIDF y similitud del coseno. (1 punto)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrAhVji-MZ4e",
        "outputId": "c79782af-79c0-4f70-b805-8324e326d2ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ¤– El lomo clÃ¡sico cuesta $9500. Â¿QuerÃ©s agregar papas o bebida? (Coincidencia con: 'Â¿CuÃ¡nto cuesta un lomo clÃ¡sico?, ')\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "\n",
        "# ğŸ”¸ Vectorizar las preguntas\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(df[\"Pregunta\"])  # Convertir preguntas a vectores numÃ©ricos\n",
        "\n",
        "# ğŸ”¹ FunciÃ³n para encontrar la mejor coincidencia\n",
        "def buscar_respuesta_tfidf(pregunta_usuario):\n",
        "    pregunta_vec = vectorizer.transform([pregunta_usuario])  # Vectorizar la nueva consulta\n",
        "    similitudes = cosine_similarity(pregunta_vec, X)  # Comparar con preguntas del dataset\n",
        "\n",
        "    mejor_indice = similitudes.argmax()  # Obtener Ã­ndice de la mejor coincidencia\n",
        "    mejor_pregunta = df[\"Pregunta\"].iloc[mejor_indice]\n",
        "    mejor_respuesta = df[\"Respuesta\"].iloc[mejor_indice]\n",
        "\n",
        "    return f\"ğŸ¤– {mejor_respuesta} (Coincidencia con: '{mejor_pregunta}')\"\n",
        "\n",
        "# ğŸ”¥ Ejemplo de uso\n",
        "pregunta_usuario = \"CuÃ¡nto cuesta el lomito?\"\n",
        "print(buscar_respuesta_tfidf(pregunta_usuario))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IG6TXW4DOJVd"
      },
      "source": [
        "3) Crear otro chatbot utilizando embeddings. Indique cuÃ¡l embedding (1 punto) pre-entrenado eligiÃ³."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGBSrn_WNwgr",
        "outputId": "ef8d7655-9c14-4ce2-b4f6-de9a7de6463a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ¤– El lomo clÃ¡sico cuesta $9500. Â¿QuerÃ©s agregar papas o bebida? (Coincidencia con: 'Â¿CuÃ¡nto cuesta un lomo clÃ¡sico?, ')\n"
          ]
        }
      ],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Cargar modelo de embeddings pre-entrenado\n",
        "modelo = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Generar embeddings para las preguntas del dataset\n",
        "preguntas_emb = modelo.encode(df[\"Pregunta\"].tolist())\n",
        "\n",
        "# FunciÃ³n para encontrar la mejor coincidencia\n",
        "def buscar_respuesta_embeddings(pregunta_usuario):\n",
        "    pregunta_vec = modelo.encode([pregunta_usuario])  # Vectorizar la nueva consulta\n",
        "    similitudes = cosine_similarity(pregunta_vec, preguntas_emb)  # Comparar con preguntas del dataset\n",
        "\n",
        "    mejor_indice = similitudes.argmax()  # Obtener Ã­ndice de la mejor coincidencia\n",
        "    mejor_pregunta = df[\"Pregunta\"].iloc[mejor_indice]\n",
        "    mejor_respuesta = df[\"Respuesta\"].iloc[mejor_indice]\n",
        "\n",
        "    return f\"ğŸ¤– {mejor_respuesta} (Coincidencia con: '{mejor_pregunta}')\"\n",
        "\n",
        "# ğŸ”¥ Ejemplo de uso\n",
        "pregunta_usuario = \"CuÃ¡nto cuesta el lomito?\"\n",
        "print(buscar_respuesta_embeddings(pregunta_usuario))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gl9ytH1LOnx6"
      },
      "source": [
        "4) Muestra ambos chatbots funcionando (1 punto)\n",
        "Adjuntar la lista de preguntas utilizadas para probar el funcionamiento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kK5ce0PTP4Ic"
      },
      "source": [
        "Cree una lista de preguntas, parecidas a las preguntas iniciales. Algunas igual y otras las fui modificando para probar la respuesta."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWPEg66TPGhP"
      },
      "outputs": [],
      "source": [
        "lista_de_pruebas = [\n",
        "    \"Â¿CuÃ¡nto cuesta el lomo?\",\n",
        "    \"Â¿CuÃ¡les son los mÃ©todos de pago?\",\n",
        "    \"Quiero pedir un lomo con cheddar.\",\n",
        "    \"Â¿CuÃ¡nto tarda el delivery?\",\n",
        "    \"Me llegÃ³ el pedido sin papas.\",\n",
        "    \"Â¿CuÃ¡l es el mejor pedido?\",\n",
        "    \"Â¿Hay opciones vegetarianas?\",\n",
        "    \"Â¿DÃ³nde se encuentran?\",\n",
        "    \"Â¿Hacen envÃ­os a Barrio JardÃ­n?\",\n",
        "    \"Â¿Tienen bebidas?\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjy3y02mO51J",
        "outputId": "d9b86451-b09c-4e2d-9c92-9c79f4787c96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Usuario: Â¿CuÃ¡nto cuesta el lomo?\n",
            "Chatbot (TF-IDF): ğŸ¤– El lomo clÃ¡sico cuesta $9500. Â¿QuerÃ©s agregar papas o bebida? (Coincidencia con: 'Â¿CuÃ¡nto cuesta un lomo clÃ¡sico?, ')\n",
            "\n",
            "Usuario: Â¿CuÃ¡les son los mÃ©todos de pago?\n",
            "Chatbot (TF-IDF): ğŸ¤– Aceptamos efectivo, tarjetas y Mercado Pago. Â¿CÃ³mo preferÃ­s pagar? (Coincidencia con: 'Â¿CuÃ¡les son los mÃ©todos de pago?, ')\n",
            "\n",
            "Usuario: Quiero pedir un lomo con cheddar.\n",
            "Chatbot (TF-IDF): ğŸ¤– Â¡Excelente elecciÃ³n! Lomo con cheddar anotado. Â¿QuerÃ©s agregar papas o bebida? (Coincidencia con: 'Quiero pedir un lomo con cheddar., ')\n",
            "\n",
            "Usuario: Â¿CuÃ¡nto tarda el delivery?\n",
            "Chatbot (TF-IDF): ğŸ¤– Depende de la zona, pero en promedio tarda entre 40 y 50 minutos. (Coincidencia con: 'Â¿CuÃ¡nto tarda el delivery?, ')\n",
            "\n",
            "Usuario: Me llegÃ³ el pedido sin papas.\n",
            "Chatbot (TF-IDF): ğŸ¤– Â¡Uy! Lamentamos el error. Por favor, indicanos quÃ© faltÃ³ y lo solucionamos de inmediato. (Coincidencia con: 'Me llegÃ³ el pedido incompleto., ')\n",
            "\n",
            "Usuario: Â¿CuÃ¡l es el mejor pedido?\n",
            "Chatbot (TF-IDF): ğŸ¤– Nuestro *Combo ClÃ¡sico* es el favorito: Lomo ClÃ¡sico + Papas + Gaseosa. Â¿QuerÃ©s probarlo? (Coincidencia con: 'Â¿CuÃ¡l es el pedido mÃ¡s vendido?, ')\n",
            "\n",
            "Usuario: Â¿Hay opciones vegetarianas?\n",
            "Chatbot (TF-IDF): ğŸ¤– No, por el momento no contamos con opciones vegetarianas, pero pensamos en implementarlas mas adelante. (Coincidencia con: 'Â¿Tienen opciones vegetarianas?, ')\n",
            "\n",
            "Usuario: Â¿DÃ³nde se encuentran?\n",
            "Chatbot (TF-IDF): ğŸ¤– Estamos en CÃ³rdoba, en zona sur. Te pasamos la direcciÃ³n exacta si la necesitÃ¡s. (Coincidencia con: 'Â¿DÃ³nde estÃ¡n ubicados?, ')\n",
            "\n",
            "Usuario: Â¿Hacen envÃ­os a Barrio JardÃ­n?\n",
            "Chatbot (TF-IDF): ğŸ¤– SÃ­, hacemos delivery en CÃ³rdoba. Â¿En quÃ© zona estÃ¡s para confirmarte el tiempo de entrega? (Coincidencia con: 'Â¿Hacen envÃ­os?, ')\n",
            "\n",
            "Usuario: Â¿Tienen bebidas?\n",
            "Chatbot (TF-IDF): ğŸ¤– Tenemos gaseosas, aguas, cervezas y jugos. Â¿QuerÃ©s ver la lista completa? (Coincidencia con: 'Â¿QuÃ© bebidas tienen?, ')\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for pregunta in lista_de_pruebas:\n",
        "    respuesta_tfidf = buscar_respuesta_tfidf(pregunta)\n",
        "    print(f\"Usuario: {pregunta}\\nChatbot (TF-IDF): {respuesta_tfidf}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egktlAsiPAvU",
        "outputId": "3eb7fa1e-71f0-499a-de50-2dfc7ce77898"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Usuario: Â¿CuÃ¡nto cuesta el lomo?\n",
            "Chatbot (Embeddings): ğŸ¤– El lomo clÃ¡sico cuesta $9500. Â¿QuerÃ©s agregar papas o bebida? (Coincidencia con: 'Â¿CuÃ¡nto cuesta un lomo clÃ¡sico?, ')\n",
            "\n",
            "Usuario: Â¿CuÃ¡les son los mÃ©todos de pago?\n",
            "Chatbot (Embeddings): ğŸ¤– Aceptamos efectivo, tarjetas y Mercado Pago. Â¿CÃ³mo preferÃ­s pagar? (Coincidencia con: 'Â¿CuÃ¡les son los mÃ©todos de pago?, ')\n",
            "\n",
            "Usuario: Quiero pedir un lomo con cheddar.\n",
            "Chatbot (Embeddings): ğŸ¤– Â¡Excelente elecciÃ³n! Lomo con cheddar anotado. Â¿QuerÃ©s agregar papas o bebida? (Coincidencia con: 'Quiero pedir un lomo con cheddar., ')\n",
            "\n",
            "Usuario: Â¿CuÃ¡nto tarda el delivery?\n",
            "Chatbot (Embeddings): ğŸ¤– Depende de la zona, pero en promedio tarda entre 40 y 50 minutos. (Coincidencia con: 'Â¿CuÃ¡nto tarda el delivery?, ')\n",
            "\n",
            "Usuario: Me llegÃ³ el pedido sin papas.\n",
            "Chatbot (Embeddings): ğŸ¤– Â¡Uy! Lamentamos el error. Por favor, indicanos quÃ© faltÃ³ y lo solucionamos de inmediato. (Coincidencia con: 'Me llegÃ³ el pedido incompleto., ')\n",
            "\n",
            "Usuario: Â¿CuÃ¡l es el mejor pedido?\n",
            "Chatbot (Embeddings): ğŸ¤– Nuestro *Combo ClÃ¡sico* es el favorito: Lomo ClÃ¡sico + Papas + Gaseosa. Â¿QuerÃ©s probarlo? (Coincidencia con: 'Â¿CuÃ¡l es el pedido mÃ¡s vendido?, ')\n",
            "\n",
            "Usuario: Â¿Hay opciones vegetarianas?\n",
            "Chatbot (Embeddings): ğŸ¤– No, por el momento no contamos con opciones vegetarianas, pero pensamos en implementarlas mas adelante. (Coincidencia con: 'Â¿Tienen opciones vegetarianas?, ')\n",
            "\n",
            "Usuario: Â¿DÃ³nde se encuentran?\n",
            "Chatbot (Embeddings): ğŸ¤– Estamos en CÃ³rdoba, en zona sur. Te pasamos la direcciÃ³n exacta si la necesitÃ¡s. (Coincidencia con: 'Â¿DÃ³nde estÃ¡n ubicados?, ')\n",
            "\n",
            "Usuario: Â¿Hacen envÃ­os a Barrio JardÃ­n?\n",
            "Chatbot (Embeddings): ğŸ¤– SÃ­, hacemos delivery en CÃ³rdoba. Â¿En quÃ© zona estÃ¡s para confirmarte el tiempo de entrega? (Coincidencia con: 'Â¿Hacen envÃ­os?, ')\n",
            "\n",
            "Usuario: Â¿Tienen bebidas?\n",
            "Chatbot (Embeddings): ğŸ¤– Tenemos gaseosas, aguas, cervezas y jugos. Â¿QuerÃ©s ver la lista completa? (Coincidencia con: 'Â¿QuÃ© bebidas tienen?, ')\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for pregunta in lista_de_pruebas:\n",
        "    respuesta_embeddings = buscar_respuesta_embeddings(pregunta)\n",
        "    print(f\"Usuario: {pregunta}\\nChatbot (Embeddings): {respuesta_embeddings}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXOkaBrzQ-06"
      },
      "source": [
        "5) AÃ±ade tus conclusiones de todo lo realizado (2 punto)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6awk7aB5RIlG"
      },
      "source": [
        "Al ir haciendo todos los pasos para crear el chatbot creo que fui logrando buenos resultados, lo cual me sorprendio mas que nada el poder de relacion entre las preguntas.\n",
        "\n",
        "Como primera medida cree un dataset en una planilla de excel, donde resulta mas facil modificarla en un futuro por cualquier persona y no quede solamente en el codigo.\n",
        "\n",
        "La implementacion de tf-idf resulto bastante buena para lo que necesitaba, al usar un modelo de embeding estuve viendo varios, consulte en la IA de copilot y me sugirio hacerlo con all-MiniLM-L6-v2 el cual es rapido y preciso.\n",
        "\n",
        "Para mi caso con las preguntas no note cambios significantes. Auque fui probando cambiar las preguntas y en el modelo tf-idf no las reconocia.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMiGowk5Rgjg"
      },
      "source": [
        "6) BONUS: usa lo realizado en 1 y 3 para crear un chatbot RAG. (2 puntos)\n",
        "Utiliza un modelo LLM pre-entrenado.\n",
        "\n",
        "Este punto no es obligatorio de realizar para quienes quieran regularizar / recuperar y luego rendirÃ¡n en mesa.\n",
        "\n",
        "Para quienes tienen condiciones para promocionar (han realizado y entregado los TPs a tiempo) la resoluciÃ³n de este ejercicio serÃ¡ tenida en cuenta para sumar a la promociÃ³n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "-I30fw3BtENo",
        "outputId": "2b20820b-4937-4de8-924a-8f7f00a566e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m19.3/19.3 MB\u001b[0m \u001b[31m74.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m92.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m75.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m55.7/55.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m196.2/196.2 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m118.5/118.5 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m100.0/100.0 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m107.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m453.1/453.1 kB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install chromadb --quiet\n",
        "!pip install transformers --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "moDrYTiEcQUS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from chromadb import Client\n",
        "from transformers import pipeline\n",
        "\n",
        "# Generar embeddings con SentenceTransformer\n",
        "modelo_embeddings = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "preguntas_emb = modelo_embeddings.encode(df[\"Pregunta\"].tolist())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-yixkUBiDzu"
      },
      "outputs": [],
      "source": [
        "# Crear y llenar la colecciÃ³n en ChromaDB\n",
        "chroma_client = Client()\n",
        "chroma_collection = chroma_client.create_collection(name=\"chatbot_lomiteria\", get_or_create=True)\n",
        "\n",
        "for i, pregunta in enumerate(df[\"Pregunta\"]):\n",
        "    chroma_collection.add(\n",
        "        ids=[str(i)],\n",
        "        embeddings=[preguntas_emb[i]],\n",
        "        metadatas=[{\"pregunta\": pregunta, \"respuesta\": df[\"Respuesta\"].iloc[i]}]\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258,
          "referenced_widgets": [
            "754cb9f07e434642bb2fe1e91aa2b004",
            "1c6bd89a265c4a63aa07b0fbc62083c7",
            "c8937f986ae64a608ab043e10e1424b0",
            "9fbeed11d6154c219fede796742eb267",
            "9b90d4593f5044a9956448966bcb2691",
            "312285faf4ab490b82d67a9e2e0ec1aa",
            "f9714ed0c54048b7b568ed6740012a3c",
            "66f2d22db3a543338e6f404a1e1a69d7",
            "e84437c451f948dc821d0f97aab76b26",
            "066b23e2d1b449658c965aa5fa4166ab",
            "36277aa3b93e42b9ae51601c2293aa7e",
            "0585ea52a1b345658dccc3a018704b17",
            "1a2bffc57a454523867c9075abe0e7bc",
            "257c078bc5094569944f5d582d4f6c7f",
            "f52ce639443540cd9ab1aed15d703363",
            "35bb6ea56e1a48fba9755be4a41804c0",
            "f65c75159e3348a28a3f0d373acaa11d",
            "520410373add4b6d827f0ac79e93dfa4",
            "e87cfc02cf854a4bb946288ea0568f17",
            "1e79856505434321b10f7ebbdad82020",
            "632177ba661b42a4b4898b680d6cb783",
            "c8d5f5d7aa7a4bcf998edca46ff8cabd",
            "44380d351086499f858f8cf2d01433dd",
            "453a17e6d80742179eb819f4b2a87f70",
            "10cce46db3204feaa858e414e43fd0be",
            "2aa6220d2e2c40d68d919a23f340ddd3",
            "85b49140208d40e58c23f2479ac81664",
            "8942da5faff748e3af0b4ac374a167a7",
            "15f0d4455cbd4c1a8ce64c40a8103e1e",
            "3f6eed85805c45f481d624115c88f30a",
            "06e0c8564d1b4045ab8d0109690f78b0",
            "66cea69650ae436290f42b405eeb22f2",
            "9683943d47d44cdfa5ff02ca0f942e10",
            "04f2da2e45bd43e19ad74852f673b72f",
            "c0c103c9ce29438cab40471a8b7f4933",
            "eeea63d676ab4130b6b6eebb7bde735a",
            "a847219b09894991b137ae08b7d385b3",
            "e48e163186e34f539ad1980529def80d",
            "9b6f81a5b60a4bbcae9233768432c21d",
            "8b7e9b079cac424ca133e2190a3e4c1b",
            "ca5089e4a34342fc8a3a5659801541b0",
            "d19b03c3f19b4ea1bf21c2b89fe4e908",
            "985a2dd31e9948a5bd2e434db59e8e94",
            "60959ba627e34dcfbc231bbed3f9af5c",
            "04aad7f732a742d3aba88876ae100252",
            "0ca977f2c9ca426c92b95f7fbcb6626c",
            "98b4bfa5fcdd473883214dfd43ebbd16",
            "051ea62796f044cd819e3c3a3ff8b8ed",
            "283256dc45e740cb973c1ce93014b1ab",
            "a7de12c2c21c4525895fcf525e4da6e8",
            "b11166ba85fa484c87e7fcfa8d05a0f3",
            "5f291a18f1cc4e789ff3e595e72e9e79",
            "d972d536c019461ba6f442c661c1b3e9",
            "a58777f00cca4a3c8b75822c2060c6a9",
            "4056350f34f14e18bb5a3d1b93b41f3d",
            "f1e1370dee10407d8be0a412a41c2023",
            "33b009443e4144669feb3ba743788eed",
            "933ad6ca0ce1430299259900e78aeafa",
            "e6c41f16331045a589ead75aaa1d3f20",
            "90c2ccb3a45c47ebac19d36d8473d761",
            "f8d5fd167a9d4da9855042265fb96949",
            "9d6fff94c6de4d5cb58010a0c515a762",
            "37e3316311e048acad654e52c07e61c2",
            "f8e74179f63e48ae949c53ba1086e934",
            "668e2c15bd0246b68337cb34f85651bf",
            "b0febc5ed2344d90b5d6f57a44b91e7e",
            "335bcaed4521440baf1e9e07c8f105e6",
            "e6a18fdff33147e29985fb52217f7d2e",
            "ec75a0506168446fa9a706c06f503854",
            "33920285677c4b89955990bd83874dab",
            "9f4f5a16b17c408b9c4c143df4ede2d2",
            "a3b61d0746a84c479965bc6dde044535",
            "c660b76d2b4f4e819ca221463a2bdb13",
            "43e49e00c29140c29dd4a68ad6c20fd0",
            "b5530876f21b41cabfc660d4db5f37c0",
            "bbec7f8cf0394effb8714f8389dd428f",
            "aae3599840334360bf3789b7fbc45f7b"
          ]
        },
        "collapsed": true,
        "id": "3OVbJojIiJHo",
        "outputId": "5a4b60e5-8dcf-4370-ca61-c8a07d5b5ea9"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "754cb9f07e434642bb2fe1e91aa2b004",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0585ea52a1b345658dccc3a018704b17",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/308M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "44380d351086499f858f8cf2d01433dd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "04f2da2e45bd43e19ad74852f673b72f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "04aad7f732a742d3aba88876ae100252",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f1e1370dee10407d8be0a412a41c2023",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "335bcaed4521440baf1e9e07c8f105e6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Cargar modelo LLM HuggingFace (Flan-T5 small)\n",
        "modelo_llm = pipeline(\"text2text-generation\", model=\"google/flan-t5-small\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mGG16YUMiNSq"
      },
      "outputs": [],
      "source": [
        "# FunciÃ³n RAG para responder\n",
        "def responder_chatbot_rag(pregunta_usuario):\n",
        "    resultados = chroma_collection.query(\n",
        "        query_embeddings=modelo_embeddings.encode([pregunta_usuario]),\n",
        "        n_results=1\n",
        "    )\n",
        "\n",
        "    mejor_pregunta = resultados[\"metadatas\"][0][0][\"pregunta\"]\n",
        "    mejor_respuesta = resultados[\"metadatas\"][0][0][\"respuesta\"]\n",
        "\n",
        "    prompt = (\n",
        "        f\"Pregunta: {pregunta_usuario}\\n\"\n",
        "        f\"Respuesta sugerida: {mejor_respuesta}\\n\"\n",
        "        \"Eres un chatbot especializado en atenciÃ³n al cliente de una lomiterÃ­a. Responde de manera clara, concisa y en espaÃ±ol\"\n",
        "    )\n",
        "\n",
        "    salida = modelo_llm(prompt, max_new_tokens=50)[0][\"generated_text\"]\n",
        "    return f\"ğŸ¤– {salida} (Basado en: '{mejor_pregunta}')\"\n",
        "    # Generar respuesta final con LLM\n",
        "    salida = modelo_llm(prompt, max_new_tokens=50)[0][\"generated_text\"]\n",
        "\n",
        "    return f\"ğŸ¤– {salida} (Basado en: '{mejor_pregunta}')\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z02GWR0KiVD1",
        "outputId": "6265ea63-f4f0-495f-9cf9-af1dc3dca7a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ¤– Question: CuÃ¡nto cuesta el lomito? Answer: El lomo clÃ¡sico cuesta $9500. QuerÃ©s agregar papas o bebid (Basado en: 'Â¿CuÃ¡nto cuesta un lomo clÃ¡sico?, ')\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Ejemplo de uso\n",
        "pregunta_usuario = \"Â¿CuÃ¡nto cuesta el lomito?\"\n",
        "print(responder_chatbot_rag(pregunta_usuario))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8DED-87W0GNe",
        "outputId": "c955a493-e1f8-494c-d994-48e1958c3839"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EscribÃ­ tu pregunta (o 'salir' para terminar): salir\n",
            "Â¡Hasta luego! ğŸ”\n"
          ]
        }
      ],
      "source": [
        "# Ejemplo de uso\n",
        "while True:\n",
        "    pregunta_usuario = input(\"EscribÃ­ tu pregunta (o 'salir' para terminar): \")\n",
        "    if pregunta_usuario.lower() == \"salir\":\n",
        "        print(\"Â¡Hasta luego! ğŸ”\")\n",
        "        break\n",
        "    print(responder_chatbot_rag(pregunta_usuario))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAcD1e7Ps-tQ"
      },
      "source": [
        "7) No olvides:\n",
        "Explicar tus decisiones y configuraciones. AÃ±adir tus conclusiones.\n",
        "Anunciar en el foro cuÃ¡l serÃ¡ tu aplicaciÃ³n y postear tu entrega y tus avances.\n",
        "Debes subir tu notebook a un repo GitHub pÃºblico de tu propiedad compartido + enlace colab.\n",
        "Documentar todo el proceso."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJWnTOQZtATW"
      },
      "source": [
        "Como conclusion final voy a retomar un poco lo que dije en el punto 5, donde los primeros modelos funcionaron bastante bien para lo que necesito.\n",
        "\n",
        "Al realizar el punto 6 tuve muchas dificultades, quise utilizar el modelo llama3 porque fue uno de los que vi, pero no logre que funcione. Con la ayuda de copilot pedi recomendaciones y me recomendo que utilizara el modelo \"google/flan-t5-small\" el cual me sirvio porque pude hacerlo funcionar.\n",
        "\n",
        "\n",
        "Para hacer mi trabajo utilice preguntas ya establecidas en el codigo lo que me resulto mas simple para probar los modelos, pero tambien se puede usar inputs para realizar las preguntas manualmente algo que hice en el ultimo modelo, pero al empezar a preguntar se va cambiando de a poco al ingles.\n",
        "Creo que necesitaria mas modelos de preguntas y respuestas, y tambien cambie el prompt como para especificar que sea solo en espaÃ±ol.\n",
        "\n",
        "Mas adelante me gustaria integrarlo a un chat real, como whatsapp para que sea una aplicacion funcional."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}